<!DOCTYPE html>
<html>
<head>
    <title>CS180 Final Project</title>
    <style>
        body {
            font-family: "Times New Roman", Times, serif;
            margin: 20px;
        }
        .image-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
            margin-top: 20px;
        }
        .image-grid img {
            width: 100%;
            height: auto;
        }
        .image-grid figcaption {
            text-align: center;
            font-size: 14px;
            margin-top: 5px;
            line-height: 1.5;
        }
        h1, h2, h3 {
            margin-bottom: 10px;
        }
        p, li {
            margin-bottom: 20px;
        }
        hr {
            margin: 30px 0;
        }
    </style>
    </style>
    <!-- MathJax script for rendering equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <h1>Final Project 1: High Dynamic Range</h1>
    <hr width="100%" size="2">

    <h2>Overview</h2>
    
    <p>The real world is in high dynamic range (HDR), which cannot be fully captured by modern cameras. However, we can combine information from multiple exposures of the same scene into a single high dynamic range radiance map, then use tone mapping to nicely display this map.</p>
    <p>Disclosure: I use starter code from <a href="https://classroom.github.com/a/u16GNLqv">Brown's CS 1290</a> GitHub classroom.</p>
    
    <h2>Base Images</h2>
    <p>Here is an example sequence of base images taken at different exposures, depicting a bonsai tree.</p>
    <img src="imgs/bonsai_exposures.png" alt="image">
    
    <h2>Radiance Map Construction</h2>
    <p>First, we need to build a HDR radiance map from several LDR exposures. Based on the logic in Sections 2.1 and 2.2 in <a href="http://www.pauldebevec.com/Research/HDR/debevec-siggraph97.pdf">Debevec and Malik 1997</a>, I wrote a function to solve for \( g = \ln(f^{-1}) \), which maps pixel values (0 to 255) to the log of exposure values. This involves solving the least-squares algorithm to estimate g and unknown log radiance values for each pixel.</p>
    
    <p>Setting Œª=50 (smoothing) gives us these results for the bonsai images:</p>
    <img src="imgs/mapped_bonsai.png" alt="image">

    <p>And here are RGB curves plotting the estimated g that we have solved for in our <code>solve_g</code> function:</p>
    <img src="imgs/gcurves.png" alt="image">
    
    <p>Now that we have estimated <code>g</code>, Equation 6 in Debevec outlines how to weight the contribution of each pixel:</p>
    <p>$$\ln E_i = \frac{\sum_{j=1}^P w(Z_{ij}) \left(g(Z_{ij}) - \ln \Delta t_j \right)}{\sum_{j=1}^P w(Z_{ij})}
$$</p>

    <p>The estimated radiance map for the bonsai tree is shown below.</p>
    <img src="imgs/bonsai_hdr_rad_map.png" alt="image">

    <h2>Tone Mapping</h2>

    <h3>Simple Global Tone Mapping</h3>
    
    <p>Let's get this radiance map to a nice visual image by using tone mapping. First, I tried a very simple global tone mapping. This entailed applying a pixel-wise transform:</p>
    <p>$$E'_i = \frac{E_i}{1 + E_i}$$</p>
    <p>then applying global scaling by subtracting the minimum value and rescaling the resulting values to lie within [0,1].</p>

    <h3>Durand Tone Mapping</h3>

    <p>For a more precise local tone mapping, I implemented a simplified version of <a href="http://people.csail.mit.edu/fredo/PUBLI/Siggraph2002/DurandBilateral.pdf">Durand 2002</a>. In short, this method first computes global intensity by averaging intensity across all color channels,  and scales colors by this intensity value. The log intensity is bilaterally filtered to get base and detail layers.</p>

    <h3>Tone Mapping Results</h3>
    <p>Here are results for the given arch, bonsai, chapel, garage, garden, house, mug, and window images. Global simple mapping is shown on the <b>left</b>, while Durand mapping is displayed on the <b>right</b>. For Durand, a gamma value of 0.7 worked best for most images (I used 0.5 for the arch and house).</p>

    <img src="imgs/mapped_arch_gamma0dot5.png" alt="image">
    <img src="imgs/mapped_bonsai_gamma0dot7.png" alt="image">
    <img src="imgs/mapped_chapel_gamma0dot7.png" alt="image">
    <img src="imgs/mapped_garage_gamma0dot7.png" alt="image">
    <img src="imgs/mapped_garden_gamma0dot7.png" alt="image">
    <img src="imgs/mapped_house_gamma0dot5.png" alt="image">
    <img src="imgs/mapped_mug_gamma0dot7.png" alt="image">
    <img src="imgs/mapped_window_gamma0dot7.png" alt="image">

    <p>The simple global scaling isn't great at capturing details, although it is better at having higher constrasts in most of the pictures. The Durand scaling preserves a lot more detail in more regions of the images, but this often results in more monotone-looking pictures.</p>
    
    <hr width="100%" size="2">
    <h2>Bells & Whistles</h2>

    <p>Here, I tested out the radiance map and the tone mapping on my own HDR images! It worked pretty well. Here are different exposures of a classroom in McCone:</p>
    
    <img src="imgs/PT1_deltat_classroom.png" alt="image">
    
    <div class="image-grid">
        <figure>
            <img src="imgs/PT1_solveg_classroom.png" alt="image">
            <figcaption>Log exposures of each RGB channel</figcaption>
        </figure>
        <figure>
            <img src="imgs/PT1_tonemapped_classroom_BGR.png" alt="image">
            <figcaption>Simple global mapping (left), Durand mapping (right)</figcaption>
        </figure>
    </div>

    <p>And also different exposures of a window in McCone.</p>

    <img src="imgs/PT1_deltat_mccone.png" alt="image">
    
    <div class="image-grid">
        <figure>
            <img src="imgs/PT1_solveg_mccone.png" alt="image">
            <figcaption>Log exposures of each RGB channel</figcaption>
        </figure>
        <figure>
            <img src="imgs/PT1_tonemapped_mccone.png" alt="image">
            <figcaption>Simple global mapping (left), Durand mapping (right)</figcaption>
        </figure>
    </div>

    <hr width="100%" size="2">
    <h2>Summary</h2>

    <p>This project helped me understand how different exposures can be combined to capture details in both lighter and darker areas of a scene. I always wondered what HDR was, due to having that option enabled on my camera as well as taking iPhone pictures. Maybe I can now use it to my advantage when capturing and editing!</p>

    <hr style="border: 2px dashed gray; background-color: transparent;">

    <h1>Final Project 2: Lightfield Camera</h1>
    <hr width="100%" size="2">
    <h2>Overview</h2>
    <p>This project uses real lightfield data to mimick the depth focusing and aperture changes of a camera. I chose the bracelet and lego images from the <a href="http://lightfield.stanford.edu/lfs.html">Stanford Light Field Archive</a> to demonstrate these operations.</p>
    <p>First, we can see that if we take the average of all the images for each object/scene - far away pixels are clear while close-up ones are blurry. This is because close-up regions change a lot more from image to image.</p>

    <div class="image-grid">
        <figure>
            <img src="imgs/average_bracelet.png" alt="image">
            <figcaption>Average Bracelet Image</figcaption>
        </figure>
        <figure>
            <img src="imgs/average_legos.png" alt="image">
            <figcaption>Average Lego Image</figcaption>
        </figure>
    </div>
    
    <hr width="100%" size="2">        
    <h2>1) Depth Refocusing</h2>
    <p>Here, we will mimick depth refocusing by shifting images with respect to the center image (image with coordinates (8, 8) on the (17, 17) grid). This information is found in the filename of each image. The amount of shift is scaled by the parameter ùê∂, which results in the average image (from adding and normalizing all the shifted images) being blurred and focused in different areas of the image.</p>

    <div class="image-grid">
        <figure>
            <img src="imgs/bracelet.gif" alt="image">
            <figcaption>Bracelet, ùê∂ values ranging from -4.5 to 1.5</figcaption>
        </figure>
        <figure>
            <img src="imgs/legos.gif" alt="image">
            <figcaption>Legos, ùê∂ values ranging from -4.0 to 4.0</figcaption>
        </figure>
    </div>

    <p>You can see in the bracelet photo that different portions become clear and then blurred as the ùê∂ value changes. This is because the bracelet in real life is laid out in a snake pattern where pieces are closer to the lightfield camera in front and pieces are farther in the back, creating a great example of mimicking depth refocusing. For the legos, the focus changes from the very front of the picture (the lego ground) to the subjects of the photo (the lego men), and then lastly to the back wall as ùê∂ increases.</p>
    
    <hr width="100%" size="2">
    <h2>2) Aperture Adjustment</h2>

    <p>We can simulate another cool feature - aperture size adjustment. A larger aperture is mimicked by averaging a large number of images sampled over the grid perpendicular to the optical axis, while a smaller aperture is mimicked by using fewer images. We choose which images are used based on the aperture radius <i>r</i>, and get a centered square of indices from a 17 by 17 matrix of images.</p>

    <div class="image-grid">
        <figure>
            <img src="imgs/bracelet_blur.gif" alt="image">
            <figcaption>Bracelet, <i>r</i> values ranging from 0 to 20</figcaption>
        </figure>
        <figure>
            <img src="imgs/legos_blur.gif" alt="image">
            <figcaption>Legos, <i>r</i> values ranging from 0 to 20</figcaption>
        </figure>
    </div>

    <p>For the bracelet, the change is most apparent in the front - as more images are included as <i>r</i> increases, the more variance there is in closer-up pixels. For the lego men, you can see that the blur becomes more global, as opposed to shifting locally in the image like in the prior section.</p>
    
    <hr width="100%" size="2">
    <h2>Bells & Whistles : Iterative Refocusing</h2>

    <p>Here, I implemented iterative refocusing. This method allows us to choose coordinates of the image we'd like to focus on, and then use a simple search procedure to find which depth value is best to scale the image shifts by in order to best align and focus on the chosen point. I chose normalized cross-correlation (from Project 1) to use as my similarity metric, which worked well. Two examples are shown below with the bracelet image. The red dot marks the input focus point.</p>

    <div class="image-grid">
        <figure>
            <img src="imgs/200_600_iterative refocus.png" alt="image">
            <figcaption>Focus point = (200, 600)</figcaption>
        </figure>
        <figure>
            <img src="imgs/250_100_iterative_refocus.png" alt="image">
            <figcaption>Focus point = (250, 100)</figcaption>
        </figure>
    </div>

    <hr width="100%" size="2">
    <h2>Bells & Whistles : Testing on my Own Images</h2>

    <p>This B&W prompt to test the lightfield methods on your own photos seemed like kind of a trick question, as a regular camera cannot capture the directional information of light rays like a lightfield camera does. However, I attempted to mimick (very broadly) how a lightfield camera might work by taking a 3x3 grid of images:</p>

    <img src="imgs/whiteboard_grid.png" alt="image">
    <figcaption>A whiteboard in Cory Hall.</figcaption>

    <div class="image-grid">
        <figure>
            <img src="imgs/average_whiteboard.png" alt="image">
            <figcaption>Average whiteboard image</figcaption>
        </figure>
        <figure>
            <img src="imgs/neg40shiftwhiteboard.png" alt="image">
            <figcaption>Depth refocusing with a ùê∂ value of -40</figcaption>
        </figure>
        <figure>
            <img src="imgs/pos40shiftwhiteboard.png" alt="image">
            <figcaption>Depth refocusing with a ùê∂ value of +40</figcaption>
        </figure>
    </div>

    <p>Although this <i>could</i> theoretically work with a much more stable camera and grid of images, in short, you can't really refocus or mimick aperture adjustments after having taken the photo(s). The reason a lightfield camera is cool is because you can computationally change focus and blur due to capturing the additional directional data of light.</p>
    
    <hr width="100%" size="2">
    <h2>Summary</h2>
    
    <p>I think this is a really cool camera concept - it's neat how seemingly complex physical and mechanical tasks like focusing and blurring can be easily simulated by simple shifting and averaging operations.</p> 
    
    <p>Thanks for reading & grading!</p>

</body>
</html>
