<!DOCTYPE html>
<html>
<head>
    <title>Project 5 - CS180</title>
    <style>
        body {
            font-family: "Times New Roman", Times, serif;
            margin: 20px;
        }
        .image-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
            margin-top: 20px;
        }
        .image-grid img {
            width: 100%;
            height: auto;
        }
        .image-grid figcaption {
            text-align: center;
            font-size: 14px;
            margin-top: 5px;
            line-height: 1.5;
        }
        h1, h2, h3 {
            margin-bottom: 10px;
        }
        p, li {
            margin-bottom: 20px;
        }
        hr {
            margin: 30px 0;
        }
    </style>
</head>

<body>
    <h1>Project 5: Fun With Diffusion Models!</h1>
    <hr width="100%" size="2">
    <h2>Part A: The Power of Diffusion Models!</h2>
    <h3>Overview</h3>
    <p>In the first half of this project, I implemented and deployed diffusion models for image generation. I also played around with diffusion sampling loops for other tasks, such as inpainting and creating optical illusions.</p>
    <hr width="100%" size="2">

    <h2>Part 0: Setup</h2>

    <p>We utilized the <a href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if">DeepFloyd IF</a> diffusion model, a two stage model trained by Stability AI. In the notebook, we instantiate DeepFloyd's <code>stage_1</code> and <code>stage_2</code> objects used for generation, as well as several text prompts for sample generation.</p>

    <p>I used the <b>random seed 517</b> and num_inference_steps = 5, 20, and 200 to generate samples for precomputed text embeddings.</p>

    <div class="image-grid">
        <figure>
            <img src="imgs/snowy_village_5.png" alt="snow">
            <figcaption>'an oil painting of a snowy mountain village'<br>num_inference_steps = 5</figcaption>
        </figure>
        <figure>
            <img src="imgs/man_hat_5.png" alt="man">
            <figcaption>'a man wearing a hat'<br>num_inference_steps = 5</figcaption>
        </figure>
        <figure>
            <img src="imgs/rocket_ship_5.png" alt="rocket">
            <figcaption>'a rocket ship'<br>num_inference_steps = 5</figcaption>
        </figure>
        <figure>
            <img src="imgs/snowy_village_20.png" alt="snow">
            <figcaption>'an oil painting of a snowy mountain village'<br>num_inference_steps = 20</figcaption>
        </figure>
        <figure>
            <img src="imgs/man_hat_20.png" alt="man">
            <figcaption>'a man wearing a hat'<br>num_inference_steps = 20</figcaption>
        </figure>
        <figure>
            <img src="imgs/rocket_ship_20.png" alt="rocket">
            <figcaption>'a rocket ship'<br>num_inference_steps = 20</figcaption>
        </figure>
        <figure>
            <img src="imgs/snowy_village_200.png" alt="snow">
            <figcaption>'an oil painting of a snowy mountain village'<br>num_inference_steps = 200</figcaption>
        </figure>
        <figure>
            <img src="imgs/man_hat_200.png" alt="man">
            <figcaption>'a man wearing a hat'<br>num_inference_steps = 200</figcaption>
        </figure>
        <figure>
            <img src="imgs/rocket_ship_200.png" alt="rocket">
            <figcaption>'a rocket ship'<br>num_inference_steps = 200</figcaption>
        </figure>
    </div>

    <p>I notice that the quality of the outputs substantially increases as the number of inference steps increases, becoming more realistic and detailed. There's sharper edges, the colors become more vibrant, and there's increased number of unique textures. For 5 inference steps, I can tell that there is still a ton of noise given the dots, and the pictures look a lot more monotone. It looks like art or drawings, while for 200 inference steps it looks more like actual photos. </p>

    <hr width="100%" size="2">

    <h2>Part 1: Sampling Loops</h2>

    <h3>1.1 Implementing the Forward Process</h3>

    <p>Here, I implemented a function to perform the forward process of diffusion, which takes in a clean image and adds noise to it. The noise level depends on timesteps, which I varied here with t = 250, 500, and 750 with an image of the Campanile.</p>

    <img src="imgs/1dot1.png" alt="image">

    <h3>1.2 Classical Denoising</h3>

    <p>A classical method of denoising images is with Gaussian blur filtering. I used <code>torchvision.transforms.functional.gaussian_blur</code> to try and remove the noise, but as you can see, this has pretty suboptimal results.</p>

    <img src="imgs/1dot2.png" alt="image">

    <h3>1.3 One-Step Denoising</h3>

    <p>Now, we'll use a pretrained diffusion model to denoise. For each of the 3 noisy images from 1.2 (t = [250, 500, 750]), I denoised the image by estimating the noise by passing it through <code>stage_1.unet</code>. This is a model that has been trained on a huge dataset of clean and noisy pairs of images. Then, I removed the noise from the noisy image to obtain an estimate of the original image. Here, I've visualized the original image, the noisy image, and the estimate of the original image for each of the timesteps we're interested in.</p>

    <img src="imgs/1dot3.png" alt="image">

    <p>Looking better! The denoising UNet model does a decent job of projecting the image onto the natural image manifold, but it does get worse (and blurrier) as you add more noise. Onto iterative denoising we go...</p>

    <h3>1.4 Iterative Denoising</h3>

    <p>Diffusion models are designed to denoise iteratively. To implement this, I created <code>strided_timesteps</code>: a list of monotonically decreasing timesteps, starting at 990, with a stride of 30, eventually reaching 0. I've displayed the noisy image every 5th loop of denoising below, along with the final predicted clean image (using iterative denoising), one-step denoised Campanile, Gaussian Blurred Campanile, and original image for reference. Hmm, there's a "clear" favorite here...</p>

    <img src="imgs/1dot4.png" alt="image">

    <div class="image-grid">
        <figure>
            <img src="imgs/1dot4iterativelydenoised.png" alt="image">
            <figcaption>Iteratively Denoised Campanile</figcaption>
        </figure>
        <figure>
            <img src="imgs/1dot4onestep.png" alt="image">
            <figcaption>One-Step Denoised Campanile</figcaption>
        </figure>
        <figure>
            <img src="imgs/1dot4gaussianblur.png" alt="image">
            <figcaption>Gaussian Blurred Campanile</figcaption>
        </figure>
    </div>

    <figure>
        <img src="imgs/campanile.png" alt="image">
        <figcaption>Original Campanile Image</figcaption>
    </figure>

    <h3>1.5 Diffusion Model Sampling</h3>

    <p>Another thing we can do with the iterative denoising is to generate images from scratch, effectively denoising pure noise. Here are 5 sampled images using the text prompt <code>"a high quality photo"</code>.</p>

    <img src="imgs/1dot5.png" alt="image">

    <h3>1.6 Classifier-Free Guidance (CFG)</h3>

    <p>The generated images in the prior section aren't super great. We can use Classifier-Free Guidance (CFG) to greatly improve image quality. Here, I implemented the <code>iterative_denoise_cfg</code> function to generate 5 images of <code>"a high quality photo"</code> with a CFG scale of Î³ = 7.</p>

    <img src="imgs/1dot6.png" alt="image">

    <p>Much better and vibrant!</p>

    <h3>1.7 Image-to-image Translation</h3>

    <p>In order to denoise an image, a diffusion model must to some extent "hallucinate" new things and be creative! Essentially, the denoising process "forces" a noisy image back onto the manifold of natural images. Here, I took the Campanile image, added some noise, and forced it back onto the image manifold without any conditioning. Here are edits of the Campanile, using the given prompt at noise levels [1, 3, 5, 7, 10, 20] with text prompt "a high quality photo" again.</p>

    <img src="imgs/1dot7dot0.png" alt="image">

    <p>Here are edits of two more of my own test images, using the same procedure.</p>

    <figure>
        <img src="imgs/1dot7_flint.png" alt="image">
        <figcaption>My favorite cartoon character, Flint Lockwood</figcaption>
    </figure>

    <figure>
        <img src="imgs/1dot7_barry.png" alt="image">
        <figcaption>And his buddy, Barry</figcaption>
    </figure>

    <h4>1.7.1 Editing Hand-Drawn and Web Images/h4>

    <p>This procedure works particularly well if we start with a nonrealistic image (e.g. painting, a sketch, some scribbles) and project it onto the natural image manifold. For this, I found one image from the web + hand drew two images to edit using the above method for noise levels [1, 3, 5, 7, 10, 20].</p>

    <figure>
        <img src="imgs/1dot7CAT.png" alt="image">
        <figcaption>Web Image of a Cat</figcaption>
    </figure>
    <figure>
        <img src="imgs/onedot7TWO.png" alt="image">
        <figcaption>Hand Drawn Image of a Girl</figcaption>
    </figure>
    <figure>
        <img src="imgs/1dot7ONE_1.png" alt="image">
        <figcaption>Hand Drawn Image of a Monitor Setup</figcaption>
    </figure>

    <h4>1.7.2 Inpainting</h4>

    <p>We can use the same procedure to implement inpainting. Given an original image and a binary mask <b>m</b>, we can create a new image that has the same content where <b>m</b> is 0, but new content wherever <b>m</b> is 1.</p>

    <figure>
        <img src="imgs/campanile_mask.png" alt="image">
        <figcaption>We will inpaint the top of the campanile.</figcaption>
    </figure>

    <div class="image-grid">
        <figure>
            <img src="imgs/172_campanile1.png" alt="snow">
        </figure>
        <figure>
            <img src="imgs/172_campanile2.png" alt="man">
        </figure>
        <figure>
            <img src="imgs/172_campanile3.png" alt="man">
        </figure>
    </div>

    <div class="image-grid">
        <figure>
            <img src="imgs/football_mask.png" alt="snow">
            <figcaption>We can also inpaint the football here!</figcaption>
        </figure>
        <figure>
            <img src="imgs/weird_football.png" alt="man">
        </figure>
    </div>

    <div class="image-grid">
        <figure>
            <img src="imgs/flint_mask.png" alt="snow">
            <figcaption>And here's Flint again.</figcaption>
        </figure>
        <figure>
            <img src="imgs/weird_flint.png" alt="man">
            <figcaption>It made his head kind of sad.</figcaption>
        </figure>
    </div>

    <h4>1.7.3 Text-Conditional Image-to-image Translation</h4>

    <p>Now, we will do the same thing as SDEdit, but guide the projection with a text prompt. This is no longer pure "projection to the natural image manifold" but also adds control using language. This is simply a matter of changing the prompt from "a high quality photo" to any precomputed prompt. I used the prompt <code>"a rocket ship"</code> for the Campanile image again, and two other cool buildings.</p>

    <figure>
        <img src="imgs/173_0_final.png" alt="man">
        <figcaption>Campanile Rocket</figcaption>
    </figure>

    <figure>
        <img src="imgs/173_1_final.png" alt="man">
        <figcaption>Building 1 Rocket</figcaption>
    </figure>

    <figure>
        <img src="imgs/173_2_final.png" alt="man">
        <figcaption>Building 2 Rocket</figcaption>
    </figure>

    <div class="image-grid">
        <figure>
            <img src="imgs/campanile.png" alt="snow">
            <figcaption>Original Campanile (for reference)</figcaption>
        </figure>
        <figure>
            <img src="imgs/173_1_og_small.png" alt="man">
            <figcaption>Original Building 1 (for reference)</figcaption>
        </figure>
        <figure>
            <img src="imgs/173_2_og_small.png" alt="man">
            <figcaption>Original Building 2 (for reference)</figcaption>
        </figure>
    </div>

    <h3>1.8 Visual Anagrams</h3>

    <div class="image-grid">
        <figure>
            <img src="imgs/oldmancampfireUP.png" alt="snow">
            <figcaption>An Oil Painting of an Old Man</figcaption>
        </figure>
        <figure>
            <img src="imgs/oldmancampfireDOWN.png" alt="man">
            <figcaption>An Oil Painting of People around a Campfire</figcaption>
        </figure>
    </div>
    <div class="image-grid">
        <figure>
            <img src="imgs/hatmandogUP.png" alt="snow">
            <figcaption>a Dog</figcaption>
        </figure>
        <figure>
            <img src="imgs/hatmandogDOWN.png" alt="man">
            <figcaption>a Man in a Hat</figcaption>
        </figure>
    </div>
    <div class="image-grid">
        <figure>
            <img src="imgs/oldmansnowDOWN.png" alt="snow">
            <figcaption>An Oil Painting of an Old Man</figcaption>
        </figure>
        <figure>
            <img src="imgs/oldmansnowUP.png" alt="man">
            <figcaption>A Snowy Village</figcaption>
        </figure>
    </div>

    <h3>1.9 Hybrid Images</h3>

    <div class="image-grid">
        <figure>
            <img src="imgs/waterfallskullUP.png" alt="man">
            <figcaption>Hybrid image of a skull and a waterfall</figcaption>
        </figure>
        <figure>
            <img src="imgs/dogrocketUP.png" alt="snow">
            <figcaption>Is it a dog? A rocket? A dog rocket?</figcaption>
        </figure>
        <figure>
            <img src="imgs/waterfallsnow.png" alt="man">
            <figcaption>Hybrid image of a waterfall and a snowy village</figcaption>
        </figure>
    </div>

    <hr width="100%" size="2">

    <h3>Part 2 (of Part A): Bells & Whistles</h3>

    <h4>Course Logo</h4>

    <h4>More Inpainting</h4>

    <hr width="100%" size="3">

    <h2>Part B: Diffusion Models from Scratch!</h2>

    <h3>Overview</h3>
    <p>In the second half of this project, I implemented and trained my own diffusion model on MNIST.</p>
    <hr width="100%" size="2">

    <h2>Part 1: Training a Single-Step Denoising UNet</h2>

    <h3>1.1 Implementing the UNet</h3>

    <p>We utilized</p>

    <img src="imgs/PT2_1dot2.png" alt="image">
    

</body>
</html>
